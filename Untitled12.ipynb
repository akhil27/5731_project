{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9zm6SnfZPB7tQqxjsPtNO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akhil27/5731_project/blob/main/Untitled12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VVzUkrLxorG",
        "outputId": "cbc657bf-bd11-4838-891d-5653f8ff6e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER OPENROUTER API KEY\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key:\").strip()\n",
        "REFERER = \"https://your-site-url.com\"\n",
        "TITLE = \"UNTL Metadata Generator\"\n",
        "\n",
        "# ===============================\n",
        "# üìÅ UNZIP LETTERS ZIP\n",
        "# ===============================\n",
        "zip_path = \"Letter.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "base_folder = os.path.join(extract_path, \"Letter\")\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def parse_metadata_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*([\\w\\s]+?):\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "# ===============================\n",
        "# ‚úçÔ∏è FEW-SHOT + SYSTEM PROMPT\n",
        "# ===============================\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Letter from Will Clayton to H. Kempner, July 15, 1943\n",
        "- Creator: Clayton, Will L. (Author)\n",
        "- Contributor: Kempner, Harris L. (Recipient)\n",
        "- Date: 1943-07-15\n",
        "- Content Description: A letter discussing cotton transactions and financial arrangements addressed to Harris Kempner.\n",
        "- Subject and Keywords: business correspondence, cotton industry, financial matters\n",
        "- Coverage: United States - Texas - Galveston County - Galveston\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Letter from D. W. Kempner to Harris Kempner, March 25, 1940\n",
        "- Creator: Kempner, Daniel Webster (Author)\n",
        "- Contributor: Kempner, Harris L. (Recipient)\n",
        "- Date: 1940-03-25\n",
        "- Content Description: A letter about business operations and family financial matters sent from Daniel Webster Kempner to Harris Kempner.\n",
        "- Subject and Keywords: business affairs, family correspondence, financial management\n",
        "- Coverage: United States - Texas - Galveston County - Galveston\n",
        "\n",
        "Example 3:\n",
        "Metadata:\n",
        "- Title: Letter from Moore and McKinley to Harris Kempner, May 5, 1941\n",
        "- Creator: Moore and McKinley (Author)\n",
        "- Contributor: Kempner, Harris L. (Recipient)\n",
        "- Date: 1941-05-05\n",
        "- Content Description: A letter from Moore and McKinley firm regarding banking operations and cotton trade issues directed to Harris Kempner.\n",
        "- Subject and Keywords: business letters, banking, cotton trade\n",
        "- Coverage: United States - Texas - Galveston County - Galveston\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical letters following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "üéØ OUTPUT 7 FIELDS:\n",
        "- Title\n",
        "- Creator\n",
        "- Contributor\n",
        "- Date\n",
        "- Content Description\n",
        "- Subject and Keywords\n",
        "- Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include recipient and letter date in \"Month Day, Year\" format.\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\"\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ GENERATE METADATA\n",
        "# ===============================\n",
        "def generate_metadata_for_document(text_prompt):\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-3.3-70b-instruct:free\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "            {\"role\": \"user\", \"content\": text_prompt.strip()}\n",
        "        ],\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1200\n",
        "    }\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API error: {e}\")\n",
        "        return \"[GPT ERROR] \" + str(e)\n",
        "\n",
        "# ===============================\n",
        "# üì¶ PROCESS DOCUMENTS\n",
        "# ===============================\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:40]   # <-- you mentioned 40 Letters, so process 40!\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    # üìù We now only send text prompt without images\n",
        "    prompt = few_shot_text + \"\\n\\nNow analyze and write metadata for a similar letter document.\"\n",
        "\n",
        "    raw_output = generate_metadata_for_document(prompt)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_metadata_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"letters_metadata_llama.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S4bUwLm60Zlf",
        "outputId": "7bc3447d-a44a-4ee9-b10e-c32643d21efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: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\n",
            "\n",
            "üìÇ Processing: Letter 1\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 10\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 11\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 12\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 13\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 14\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 15\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 16\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 17\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 18\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 19\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 2\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 20\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 21\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 22\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 23\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 24\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 25\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 26\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 27\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 28\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 29\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 3\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 30\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 31\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 32\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 33\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 34\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 35\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 36\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 37\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 38\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 39\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 4\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 40\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 5\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 6\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 7\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 8\n",
            "‚ùå OpenRouter API error: 'choices'\n",
            "\n",
            "üìÇ Processing: Letter 9\n",
            "‚ùå OpenRouter API error: 'choices'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_aa49b658-2b89-4603-b831-f9334afc06b5\", \"letters_metadata_llama.csv\", 2347)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install tesseract-ocr\n",
        "!pip install pytesseract\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "063HuuW2YrIM",
        "outputId": "a85080ba-05cb-4106-860b-49fea1bb3532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.2.1)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "# ===============================\n",
        "# üìÅ UNZIP DOCUMENTS\n",
        "# ===============================\n",
        "zip_path = \"Pamphlet.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "base_folder = os.path.join(extract_path, \"Pamphlet\")\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "# ===============================\n",
        "# ‚úçÔ∏è FEW-SHOT + SYSTEM PROMPT\n",
        "# ===============================\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: 17th Annual Country Club Golf Tournament [Flyer]\n",
        "- Creator: Lopez, J. Lewis (Tournament Chairman)\n",
        "- Subject: Golf tournaments, country clubs, leisure activities, Galveston\n",
        "- Description: A flyer providing a schedule for the 17th annual Country Club Golf Tournament held in Galveston, Texas. It includes dates, social events, and organizational information associated with the event.\n",
        "- Publisher: Rosenberg Library\n",
        "- Contributor: Kempner, Harris Leon (Correspondent)\n",
        "- Date: 1960-08-11/1960-08-14\n",
        "- Type: Text\n",
        "- Format: Pamphlet; [1] page, 26 x 21 cm; JPEG\n",
        "- Identifier: ark:/67531/metapth1420610\n",
        "- Source: Harris and Eliza Kempner Collection, MS 80-0002\n",
        "- Language: English\n",
        "- Relation: Part of the Personal Papers (MS 80-0002) series\n",
        "- Coverage: Galveston, Texas, United States\n",
        "- Rights: Public Domain\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: ABC Bulb and Seed Specials for Spring Planting\n",
        "- Creator: American Bulb Company\n",
        "- Subject: Cannas, lilies, seedlings, flowers, gardening, spring planting\n",
        "- Description: A single-page pamphlet promoting seasonal flower seeds and bulbs from the American Bulb Company. It features prices, illustrations of flowers, and is addressed to D. W. Kempner.\n",
        "- Publisher: Rosenberg Library\n",
        "- Contributor: Kempner, Daniel W. (Daniel Webster), 1877‚Äì1956\n",
        "- Date: 1950\n",
        "- Type: Text\n",
        "- Format: Pamphlet; 1 page; 16 x 23 cm; JPEG\n",
        "- Identifier: ark:/67531/metapth1349947\n",
        "- Source: Harris and Eliza Kempner Collection, MS 80-0002\n",
        "- Language: English\n",
        "- Relation: Part of the Personal Papers (MS 80-0002) series\n",
        "- Coverage: United States\n",
        "- Rights: Public Domain\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical letters following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "üéØ OUTPUT 7 FIELDS:\n",
        "- Title\n",
        "- Creator\n",
        "- Contributor\n",
        "- Date\n",
        "- Content Description\n",
        "- Subject and Keywords\n",
        "- Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include recipient and letter date in \"Month Day, Year\" format.\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\"\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-maverick:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"pamphlets_metadata_llama.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'pamphlets_metadata_llama.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "feQUDIpuYwhO",
        "outputId": "a3836167-ce9d-4262-a74a-a283a8f0a4aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: Pamphlet1\n",
            "\n",
            "üìÇ Processing: Pamphlet10\n",
            "\n",
            "üìÇ Processing: Pamphlet11\n",
            "\n",
            "üìÇ Processing: Pamphlet12\n",
            "\n",
            "üìÇ Processing: Pamphlet13\n",
            "\n",
            "üìÇ Processing: Pamphlet14\n",
            "\n",
            "üìÇ Processing: Pamphlet15\n",
            "\n",
            "üìÇ Processing: Pamphlet16\n",
            "\n",
            "üìÇ Processing: Pamphlet17\n",
            "\n",
            "üìÇ Processing: Pamphlet18\n",
            "\n",
            "üìÇ Processing: Pamphlet19\n",
            "\n",
            "üìÇ Processing: Pamphlet2\n",
            "\n",
            "üìÇ Processing: Pamphlet20\n",
            "\n",
            "üìÇ Processing: Pamphlet3\n",
            "\n",
            "üìÇ Processing: Pamphlet4\n",
            "\n",
            "üìÇ Processing: Pamphlet5\n",
            "\n",
            "üìÇ Processing: Pamphlet6\n",
            "\n",
            "üìÇ Processing: Pamphlet7\n",
            "\n",
            "üìÇ Processing: Pamphlet8\n",
            "\n",
            "üìÇ Processing: Pamphlet9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f339b8ba-2619-44dc-af93-8937f05e842e\", \"pamphlets_metadata_llama.csv\", 37044)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'pamphlets_metadata_llama.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "# ===============================\n",
        "# üìÅ UNZIP DOCUMENTS\n",
        "# ===============================\n",
        "# ===============================\n",
        "# üìÅ UNZIP DOCUMENTS\n",
        "# ===============================\n",
        "zip_path = \"Pamphlet.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Pamphlet\")):\n",
        "    base_folder = os.path.join(extract_path, \"Pamphlet\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: 17th Annual Country Club Golf Tournament [Flyer]\n",
        "- Creator: Lopez, J. Lewis (Tournament Chairman)\n",
        "- Subject: Golf tournaments, country clubs, leisure activities, Galveston\n",
        "- Description: A flyer providing a schedule for the 17th annual Country Club Golf Tournament held in Galveston, Texas. It includes dates, social events, and organizational information associated with the event.\n",
        "- Publisher: Rosenberg Library\n",
        "- Contributor: Kempner, Harris Leon (Correspondent)\n",
        "- Date: 1960-08-11/1960-08-14\n",
        "- Type: Text\n",
        "- Format: Pamphlet; [1] page, 26 x 21 cm; JPEG\n",
        "- Identifier: ark:/67531/metapth1420610\n",
        "- Source: Harris and Eliza Kempner Collection, MS 80-0002\n",
        "- Language: English\n",
        "- Relation: Part of the Personal Papers (MS 80-0002) series\n",
        "- Coverage: Galveston, Texas, United States\n",
        "- Rights: Public Domain\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: ABC Bulb and Seed Specials for Spring Planting\n",
        "- Creator: American Bulb Company\n",
        "- Subject: Cannas, lilies, seedlings, flowers, gardening, spring planting\n",
        "- Description: A single-page pamphlet promoting seasonal flower seeds and bulbs from the American Bulb Company. It features prices, illustrations of flowers, and is addressed to D. W. Kempner.\n",
        "- Publisher: Rosenberg Library\n",
        "- Contributor: Kempner, Daniel W. (Daniel Webster), 1877‚Äì1956\n",
        "- Date: 1950\n",
        "- Type: Text\n",
        "- Format: Pamphlet; 1 page; 16 x 23 cm; JPEG\n",
        "- Identifier: ark:/67531/metapth1349947\n",
        "- Source: Harris and Eliza Kempner Collection, MS 80-0002\n",
        "- Language: English\n",
        "- Relation: Part of the Personal Papers (MS 80-0002) series\n",
        "- Coverage: United States\n",
        "- Rights: Public Domain\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical letters following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Letter from A to B, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"pamphlets_metadata_llama.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'pamphlets_metadata_llama.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "1g10kOlY3Qu6",
        "outputId": "bf4e52b1-ff54-47e6-84cb-f233d8da9e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-afcf819f734e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# üîë ENTER YOUR OPENROUTER CREDENTIALS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# ===============================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mOPENROUTER_API_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your OpenRouter API Key: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mREFERER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://your-site-url.com\"\u001b[0m  \u001b[0;31m# Optional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mTITLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"UNTL Metadata Generator\"\u001b[0m       \u001b[0;31m# Optional for ranking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Letter.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Letter\")):\n",
        "    base_folder = os.path.join(extract_path, \"Letter\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Letter from Will Clayton to H. Kempner, July 15, 1943\n",
        "- Creator: Clayton, Will L. (Author)\n",
        "- Contributor: Kempner, Harris L. (Recipient)\n",
        "- Date: 1943-07-15\n",
        "- Content Description: A letter discussing cotton transactions and financial arrangements addressed to Harris Kempner.\n",
        "- Subject and Keywords: business correspondence, cotton industry, financial matters\n",
        "- Coverage: United States - Texas - Galveston County - Galveston\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Letter from D. W. Kempner to Harris Kempner, March 25, 1940\n",
        "- Creator: Kempner, Daniel Webster (Author)\n",
        "- Contributor: Kempner, Harris L. (Recipient)\n",
        "- Date: 1940-03-25\n",
        "- Content Description: A letter about business operations and family financial matters sent from Daniel Webster Kempner to Harris Kempner.\n",
        "- Subject and Keywords: business affairs, family correspondence, financial management\n",
        "- Coverage: United States - Texas - Galveston County - Galveston\n",
        "\n",
        "Example 3:\n",
        "Metadata:\n",
        "- Title: Letter from Moore and McKinley to Harris Kempner, May 5, 1941\n",
        "- Creator: Moore and McKinley (Author)\n",
        "- Contributor: Kempner, Harris L. (Recipient)\n",
        "- Date: 1941-05-05\n",
        "- Content Description: A letter from Moore and McKinley firm regarding banking operations and cotton trade issues directed to Harris Kempner.\n",
        "- Subject and Keywords: business letters, banking, cotton trade\n",
        "- Coverage: United States - Texas - Galveston County - Galveston\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical letters following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Letter from A to B, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-maverick:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"letters_metadata_llama.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'letters_metadata_llama.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "id": "mJw1sNPM1HbZ",
        "outputId": "5c501c23-43b4-4a63-c86f-c95d42a2acf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: Letter 1\n",
            "\n",
            "üìÇ Processing: Letter 10\n",
            "\n",
            "üìÇ Processing: Letter 11\n",
            "\n",
            "üìÇ Processing: Letter 12\n",
            "\n",
            "üìÇ Processing: Letter 13\n",
            "\n",
            "üìÇ Processing: Letter 16\n",
            "\n",
            "üìÇ Processing: Letter 17\n",
            "\n",
            "üìÇ Processing: Letter 18\n",
            "\n",
            "üìÇ Processing: Letter 2\n",
            "\n",
            "üìÇ Processing: Letter 20\n",
            "\n",
            "üìÇ Processing: Letter 21\n",
            "\n",
            "üìÇ Processing: Letter 22\n",
            "\n",
            "üìÇ Processing: Letter 23\n",
            "\n",
            "üìÇ Processing: Letter 24\n",
            "\n",
            "üìÇ Processing: Letter 25\n",
            "\n",
            "üìÇ Processing: Letter 26\n",
            "\n",
            "üìÇ Processing: Letter 27\n",
            "\n",
            "üìÇ Processing: Letter 28\n",
            "\n",
            "üìÇ Processing: Letter 29\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3f823cac-0cc5-45d2-8eac-b25cf72f836e\", \"letters_metadata_llama.csv\", 20744)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'letters_metadata_llama.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Photographs.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Photographs\")):\n",
        "    base_folder = os.path.join(extract_path, \"Photographs\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Photograph of a Child Sitting and Holding a Dog\n",
        "- Creator: Unknown\n",
        "- Subject and Keywords: children, pets, outdoor activities\n",
        "- Content Description: A black-and-white photograph showing a young child seated on the ground, smiling while holding a small dog, with a rustic background.\n",
        "- Date: 19XX\n",
        "- Coverage: United States\n",
        "- Format: 1 photograph: ; 8 x 8 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Photograph of a Family Gathering Outdoors\n",
        "- Creator: Unknown\n",
        "- Subject and Keywords: family, group portraits, leisure activities\n",
        "- Content Description: A photograph capturing a family of five posing together in a grassy field, dressed in early 20th-century attire.\n",
        "- Date: 19XX\n",
        "- Coverage: Germany - Berlin - Berlin\n",
        "- Format: 1 photograph: 9 x 14 cm\n",
        "\n",
        "Example 3:\n",
        "Metadata:\n",
        "- Title: Homestead Buildings with Barn and Yard\n",
        "- Creator: Unknown\n",
        "- Subject and Keywords: agriculture, homesteads, barns\n",
        "- Content Description: A rural homestead photograph showing a central muddy ground surrounded by several wooden buildings, indicative of farming activity.\n",
        "- Date: 19XX\n",
        "- Coverage: United States\n",
        "- Format: 1 photograph: 13 x 18 cm\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical photohraphs following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Photograph title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"photographs_metadata_llama.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'photographs_metadata_llama.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "XRs8eHDECawq",
        "outputId": "e74266c8-7595-4a99-a1c7-65733be79dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: Letter 1\n",
            "\n",
            "üìÇ Processing: Letter 10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ff478e69a477>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mresize_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0mextracted_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_text_from_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0mraw_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_metadata_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-ff478e69a477>\u001b[0m in \u001b[0;36mextract_text_from_images\u001b[0;34m(image_paths)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytesseract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mfull_text\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36mimage_to_string\u001b[0;34m(image, lang, config, nice, output_type, timeout)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m     return {\n\u001b[0m\u001b[1;32m    487\u001b[0m         \u001b[0mOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBYTES\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0mOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDICT\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0mOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBYTES\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0mOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDICT\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m         \u001b[0mOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTRING\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m     }[output_type]()\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36mrun_and_get_output\u001b[0;34m(image, extension, lang, config, nice, timeout, return_bytes)\u001b[0m\n\u001b[1;32m    350\u001b[0m         }\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mrun_tesseract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         return _read_output(\n\u001b[1;32m    354\u001b[0m             \u001b[0;34mf\"{kwargs['output_filename_base']}{extsep}{extension}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36mrun_tesseract\u001b[0;34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTesseractNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtimeout_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror_string\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTesseractError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36mtimeout_manager\u001b[0;34m(proc, seconds)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m                 \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2113\u001b[0m                             'failed to raise TimeoutExpired.')\n\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Image.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Image\")):\n",
        "    base_folder = os.path.join(extract_path, \"Image\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Advertisement for Amana Air Conditioner\n",
        "- Creator: Amana Refrigeration Incorporated\n",
        "- Subject and Keywords: air conditioning, advertisements, consumer products\n",
        "- Content Description: A promotional pamphlet detailing technical specifications and product features for Amana brand residential air conditioners.\n",
        "- Date: 1955~\n",
        "- Coverage: United States - Iowa - Iowa County - Middle Amana\n",
        "- Format: 1 page; 28 x 22 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Sugar Chart for the Year 1952\n",
        "- Creator: H. H. Pike & Son\n",
        "- Subject and Keywords: sugar trade, historical charts, commerce\n",
        "- Content Description: A commercial sugar chart showing commodity prices and trends for the year 1952, intended for businesses and traders.\n",
        "- Date: 1952\n",
        "- Coverage: United States\n",
        "- Format: 1 page; 36 x 49 cm\n",
        "\n",
        "Example 3:\n",
        "Metadata:\n",
        "- Title: Membership Notice from Galveston Chamber of Commerce\n",
        "- Creator: Galveston Chamber of Commerce\n",
        "- Subject and Keywords: business communications, chamber of commerce, local trade\n",
        "- Content Description: A printed notice sent to members of the Galveston Chamber of Commerce regarding upcoming meetings and events.\n",
        "- Date: 1950-09\n",
        "- Coverage: United States - Texas - Galveston County - Galveston\n",
        "- Format: 2 pages; 28 cm; 8 x 15 cm\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical Images following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Image title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"images_metadata_llama.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'images_metadata_llama.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        },
        "id": "U7nqtIExaS02",
        "outputId": "738372aa-98a9-47ae-9ebb-0de430cba88c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: Image1\n",
            "\n",
            "üìÇ Processing: Image10\n",
            "\n",
            "üìÇ Processing: Image11\n",
            "\n",
            "üìÇ Processing: Image12\n",
            "\n",
            "üìÇ Processing: Image13\n",
            "\n",
            "üìÇ Processing: Image14\n",
            "\n",
            "üìÇ Processing: Image15\n",
            "\n",
            "üìÇ Processing: Image16\n",
            "\n",
            "üìÇ Processing: Image17\n",
            "\n",
            "üìÇ Processing: Image18\n",
            "\n",
            "üìÇ Processing: Image2\n",
            "\n",
            "üìÇ Processing: Image3\n",
            "\n",
            "üìÇ Processing: Image4\n",
            "‚ùå OpenRouter API Error: 'choices'\n",
            "\n",
            "üìÇ Processing: Image5\n",
            "‚ùå OpenRouter API Error: 'choices'\n",
            "\n",
            "üìÇ Processing: Image6\n",
            "‚ùå OpenRouter API Error: 'choices'\n",
            "\n",
            "üìÇ Processing: Image7\n",
            "‚ùå OpenRouter API Error: 'choices'\n",
            "\n",
            "üìÇ Processing: Image8\n",
            "‚ùå OpenRouter API Error: 'choices'\n",
            "\n",
            "üìÇ Processing: Image9\n",
            "‚ùå OpenRouter API Error: 'choices'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f6f5cafd-95bc-49f4-980a-6d6e25606759\", \"images_metadata_llama.csv\", 9033)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'images_metadata_llama.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Article.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Article\")):\n",
        "    base_folder = os.path.join(extract_path, \"Article\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Newspaper Article on Cotton Trade Policies, April 12, 1943\n",
        "- Creator: The Galveston Daily News (Publisher)\n",
        "- Subject and Keywords: cotton trade, economic policies, world war II\n",
        "- Content Description: A newspaper article analyzing shifts in cotton trade regulations during wartime, focusing on Texas and Gulf Coast ports.\n",
        "- Date: 1943-04-12\n",
        "- Coverage: United States - Texas - Galveston County - Galveston\n",
        "- Format: 1 page; 28 x 22 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Editorial on Agricultural Advances, May 7, 1950\n",
        "- Creator: Houston Chronicle (Publisher)\n",
        "- Subject and Keywords: agriculture, innovation, editorial opinions\n",
        "- Content Description: An editorial piece celebrating innovations in farming machinery and their impact on Texas agricultural productivity.\n",
        "- Date: 1950-05-07\n",
        "- Coverage: United States - Texas - Harris County - Houston\n",
        "- Format: 1 page; 30 x 24 cm\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical article following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Article title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"articles_metadata_llama.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'articles_metadata_llama.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "ChMu-Uf6dW0l",
        "outputId": "ddc68d86-3e90-4612-fa4d-97a72839025a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: A1\n",
            "\n",
            "üìÇ Processing: a10\n",
            "\n",
            "üìÇ Processing: a11\n",
            "\n",
            "üìÇ Processing: a2\n",
            "\n",
            "üìÇ Processing: a3\n",
            "\n",
            "üìÇ Processing: a4\n",
            "\n",
            "üìÇ Processing: a5\n",
            "\n",
            "üìÇ Processing: a6\n",
            "\n",
            "üìÇ Processing: a7\n",
            "\n",
            "üìÇ Processing: a8\n",
            "\n",
            "üìÇ Processing: a9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f1b2baf8-1cec-466c-b815-84f17f2105c4\", \"articles_metadata_llama.csv\", 11654)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'articles_metadata_llama.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Map.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Map\")):\n",
        "    base_folder = os.path.join(extract_path, \"Map\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: 1900 Map of Galveston Island Before the Hurricane\n",
        "- Creator: Texas State Cartographic Survey (Creator)\n",
        "- Subject and Keywords: maps, galveston, geographic surveys\n",
        "- Content Description: A historical map illustrating the geography and city structures of Galveston Island before the devastation of the 1900 hurricane.\n",
        "- Date: 1900\n",
        "- Coverage: United States - Texas - Galveston County - Galveston\n",
        "- Format: 1 map; 45 x 60 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Trade Routes in the Gulf of Mexico, 1895\n",
        "- Creator: United States Department of Commerce (Creator)\n",
        "- Subject and Keywords: shipping routes, gulf of mexico, maritime trade\n",
        "- Content Description: A printed map highlighting major shipping and trade routes across the Gulf of Mexico region.\n",
        "- Date: 1895\n",
        "- Coverage: Gulf of Mexico\n",
        "- Format: 1 map; 60 x 75 cm\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical map following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Map title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"map_metadata_llama.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'map_metadata_llama.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "Mf7-OMntki6Z",
        "outputId": "1bc6e33e-27fd-4291-d9c4-511ce7650137"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: map1\n",
            "\n",
            "üìÇ Processing: map10\n",
            "\n",
            "üìÇ Processing: map11\n",
            "\n",
            "üìÇ Processing: map12\n",
            "\n",
            "üìÇ Processing: map13\n",
            "\n",
            "üìÇ Processing: map14\n",
            "\n",
            "üìÇ Processing: map15\n",
            "\n",
            "üìÇ Processing: map16\n",
            "\n",
            "üìÇ Processing: map17\n",
            "\n",
            "üìÇ Processing: map18\n",
            "\n",
            "üìÇ Processing: map19\n",
            "\n",
            "üìÇ Processing: map2\n",
            "\n",
            "üìÇ Processing: map20\n",
            "\n",
            "üìÇ Processing: map3\n",
            "\n",
            "üìÇ Processing: map4\n",
            "\n",
            "üìÇ Processing: map5\n",
            "\n",
            "üìÇ Processing: map6\n",
            "\n",
            "üìÇ Processing: map7\n",
            "\n",
            "üìÇ Processing: map8\n",
            "\n",
            "üìÇ Processing: map9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f5b70663-7f50-49ff-a6b3-404c0ec538ba\", \"map_metadata_llama.csv\", 4819)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'map_metadata_llama.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Paper.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Paper\")):\n",
        "    base_folder = os.path.join(extract_path, \"Paper\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Research Paper on Agricultural Export Strategies, 1947\n",
        "- Creator: Kempner, Harris L. (Author)\n",
        "- Subject and Keywords: agriculture exports, business strategies, post-war economy\n",
        "- Content Description: A typewritten research paper discussing approaches to expanding Texas agricultural exports after World War II.\n",
        "- Date: 1947-09-15\n",
        "- Coverage: United States - Texas\n",
        "- Format: 15 pages; 21 x 28 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Personal Letter on Business Challenges, June 2, 1955\n",
        "- Creator: Kempner, Daniel W. (Author)\n",
        "- Subject and Keywords: personal correspondence, business hardships, family letters\n",
        "- Content Description: A personal paper discussing financial challenges and family matters in the cotton industry.\n",
        "- Date: 1955-06-02\n",
        "- Coverage: United States - Texas - Galveston County\n",
        "- Format: 3 pages; 20 x 25 cm\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical papers following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Paper title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"Paper_metadata_llama.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'Paper_metadata_llama.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "CHSBfEi5pdks",
        "outputId": "33988a95-7c8f-4ab8-ddec-7e03fcce6613"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: p1\n",
            "\n",
            "üìÇ Processing: p10\n",
            "\n",
            "üìÇ Processing: p11\n",
            "\n",
            "üìÇ Processing: p12\n",
            "\n",
            "üìÇ Processing: p13\n",
            "\n",
            "üìÇ Processing: p14\n",
            "\n",
            "üìÇ Processing: p2\n",
            "\n",
            "üìÇ Processing: p3\n",
            "\n",
            "üìÇ Processing: p4\n",
            "\n",
            "üìÇ Processing: p5\n",
            "\n",
            "üìÇ Processing: p6\n",
            "\n",
            "üìÇ Processing: p7\n",
            "\n",
            "üìÇ Processing: p8\n",
            "\n",
            "üìÇ Processing: p9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_abf8e076-ced3-4cb6-9d0f-e6cac60e6349\", \"Paper_metadata_llama.csv\", 9195)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'Paper_metadata_llama.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Legislative document.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Legislative document\")):\n",
        "    base_folder = os.path.join(extract_path, \"Legislative document\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Texas Senate Bill on Cotton Regulation, 1939\n",
        "- Creator: Texas Legislature (Author)\n",
        "- Subject and Keywords: legislation, cotton regulation, agriculture policy\n",
        "- Content Description: A legislative document outlining regulations for cotton production limits and export incentives within Texas.\n",
        "- Date: 1939-03-21\n",
        "- Coverage: United States - Texas\n",
        "- Format: 8 pages; 21 x 30 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Congressional Report on Southern Agriculture, 1945\n",
        "- Creator: United States Congress (Author)\n",
        "- Subject and Keywords: congressional reports, southern agriculture, federal support\n",
        "- Content Description: A report prepared for the U.S. Congress discussing the state of agriculture in the southern states and recommending financial aid.\n",
        "- Date: 1945-07-10\n",
        "- Coverage: United States - Southern States\n",
        "- Format: 20 pages; 22 x 28 cm\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical Legislative document following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Legislative document title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"Legislative document_metadata_llama.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'Legislative document_metadata_llama.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "6BQStIIEruN8",
        "outputId": "941a0124-d93b-4337-f705-159ae29c5d70"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: ld1\n",
            "\n",
            "üìÇ Processing: ld10\n",
            "\n",
            "üìÇ Processing: ld11\n",
            "\n",
            "üìÇ Processing: ld12\n",
            "\n",
            "üìÇ Processing: ld13\n",
            "\n",
            "üìÇ Processing: ld14\n",
            "\n",
            "üìÇ Processing: ld15\n",
            "\n",
            "üìÇ Processing: ld2\n",
            "\n",
            "üìÇ Processing: ld3\n",
            "\n",
            "üìÇ Processing: ld4\n",
            "\n",
            "üìÇ Processing: ld5\n",
            "\n",
            "üìÇ Processing: ld6\n",
            "\n",
            "üìÇ Processing: ld7\n",
            "\n",
            "üìÇ Processing: ld8\n",
            "\n",
            "üìÇ Processing: ld9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5e1dc013-f364-4bd0-9cab-01a62526346e\", \"Legislative document_metadata_llama.csv\", 14791)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'Legislative document_metadata_llama.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Legislative document.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Legislative document\")):\n",
        "    base_folder = os.path.join(extract_path, \"Legislative document\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Texas Senate Bill on Cotton Regulation, 1939\n",
        "- Creator: Texas Legislature (Author)\n",
        "- Subject and Keywords: legislation, cotton regulation, agriculture policy\n",
        "- Content Description: A legislative document outlining regulations for cotton production limits and export incentives within Texas.\n",
        "- Date: 1939-03-21\n",
        "- Coverage: United States - Texas\n",
        "- Format: 8 pages; 21 x 30 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Congressional Report on Southern Agriculture, 1945\n",
        "- Creator: United States Congress (Author)\n",
        "- Subject and Keywords: congressional reports, southern agriculture, federal support\n",
        "- Content Description: A report prepared for the U.S. Congress discussing the state of agriculture in the southern states and recommending financial aid.\n",
        "- Date: 1945-07-10\n",
        "- Coverage: United States - Southern States\n",
        "- Format: 20 pages; 22 x 28 cm\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical Legislative document following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Legislative document title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"Legislative document_metadata_llama.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'Legislative document_metadata_llama.csv'\")"
      ],
      "metadata": {
        "id": "PHIETp16vCbe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}